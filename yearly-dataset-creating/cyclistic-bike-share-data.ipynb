{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab22513",
   "metadata": {
    "papermill": {
     "duration": 0.002486,
     "end_time": "2023-06-21T12:15:06.656033",
     "exception": false,
     "start_time": "2023-06-21T12:15:06.653547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Cyclistic Bike-Share Data**\n",
    "\n",
    "This Python 3 notebook is based on the [Cyclistic bike-share analysis case study](http://https://www.coursera.org/learn/google-data-analytics-capstone), which is part of the Google Data Analytics Professional Certification capstone project.\n",
    "\n",
    "# About Cyclistic\n",
    "Cyclistic is a fictitious Chicago-based bike-sharing company. As part of the project, we will analyze the bike trip data from the last 12 months to gain insights and make data-driven recommendations to improve the company's business strategy.\n",
    "\n",
    "# Data Source\n",
    "The data used in this notebook is sourced from the [divvy-tripdata S3 bucket](https://divvy-tripdata.s3.amazonaws.com/index.html). The data is made available under a specific license provided by [Motivate International Inc](https://ride.divvybikes.com/data-license-agreement) for analysis purposes by [Divvy Bikes Sharing](https://ride.divvybikes.com/).The available data starts from April 2020.\n",
    "\n",
    "# Purpose of the Notebook\n",
    "The notebook contains code that automates the process of downloading and managing the data files for the Google Data Analytics Professional Certification project. The code ensures that the data used for analysis is up to date and the data files are downloaded, extracted, and the directory size is managed to stay within the specified limit of 15GB.\n",
    "\n",
    "# Instructions\n",
    "To use this notebook, follow these steps:\n",
    "\n",
    "* Specify the **URL** for downloading the data files using the url variable.\n",
    "* Set the **oldest file month and year** using the oldest_year and oldest_month variables.\n",
    "* Set the **number of months to download** using the num_months_to_download variable.\n",
    "* Set the **maximum directory size in GB** using the max_directory_size_gb variable.\n",
    "* Run the code to download and manage the data files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c25cb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-21T12:15:06.662170Z",
     "iopub.status.busy": "2023-06-21T12:15:06.661503Z",
     "iopub.status.idle": "2023-06-21T12:15:06.669708Z",
     "shell.execute_reply": "2023-06-21T12:15:06.668925Z"
    },
    "papermill": {
     "duration": 0.013935,
     "end_time": "2023-06-21T12:15:06.672083",
     "exception": false,
     "start_time": "2023-06-21T12:15:06.658148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import datetime\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795e7774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-21T12:15:06.677992Z",
     "iopub.status.busy": "2023-06-21T12:15:06.677612Z",
     "iopub.status.idle": "2023-06-21T12:15:06.690503Z",
     "shell.execute_reply": "2023-06-21T12:15:06.689500Z"
    },
    "papermill": {
     "duration": 0.018323,
     "end_time": "2023-06-21T12:15:06.692687",
     "exception": false,
     "start_time": "2023-06-21T12:15:06.674364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_and_unzip_data(url, num_months, max_directory_size_gb):\n",
    "    current_date = datetime.datetime.now()\n",
    "    data_folder = '/kaggle/working/data/Cyclistic/'  # Specify the folder path\n",
    "    zip_folder = os.path.join(data_folder, 'zip_files')  # Folder for saving zip files\n",
    "    csv_folder = os.path.join(data_folder, 'csv_files')  # Folder for saving extracted CSV files\n",
    "    \n",
    "    # Create the data folder if it doesn't exist\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    \n",
    "    # Create the zip and csv folders if they don't exist\n",
    "    if not os.path.exists(zip_folder):\n",
    "        os.makedirs(zip_folder)\n",
    "    if not os.path.exists(csv_folder):\n",
    "        os.makedirs(csv_folder)\n",
    "    \n",
    "    # Check if the current month's data exists\n",
    "    current_year = current_date.year\n",
    "    current_month = current_date.month\n",
    "    current_file_name = f'{current_year}{current_month:02d}-divvy-tripdata.zip'\n",
    "    current_file_path = os.path.join(zip_folder, current_file_name)\n",
    "    \n",
    "    if os.path.exists(current_file_path):\n",
    "        print(f'{current_file_name} already exists. Skipping download.')\n",
    "    else:\n",
    "        print(f'{current_file_name} not found. Downloading previous month data.')\n",
    "        num_months += 1\n",
    "    \n",
    "    for i in range(num_months):\n",
    "        # Calculate the target month and year\n",
    "        target_date = current_date - datetime.timedelta(days=i*30)\n",
    "        target_year = target_date.year\n",
    "        target_month = target_date.month\n",
    "        \n",
    "        if target_year < oldest_year or (target_year == oldest_year and target_month < oldest_month):\n",
    "            print(f\"Data is only available from {oldest_month}/{oldest_year}. Unable to download files.\")\n",
    "            break\n",
    "        \n",
    "        # Format the URL for the data file\n",
    "        data_url = url.format(target_year, target_month)\n",
    "        \n",
    "        # Check if the file already exists\n",
    "        file_name = f'{target_year}{target_month:02d}-divvy-tripdata.zip'\n",
    "        file_path = os.path.join(zip_folder, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f'{file_name} already exists. Skipping download.')\n",
    "            continue\n",
    "        \n",
    "        # Download the file\n",
    "        response = requests.get(data_url)\n",
    "        \n",
    "        # Save the file\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        \n",
    "        # Extract the ZIP file contents\n",
    "        try:\n",
    "            zip_file = zipfile.ZipFile(file_path)\n",
    "            for member in zip_file.namelist():\n",
    "                if member.endswith('.csv'):\n",
    "                    zip_file.extract(member, csv_folder)\n",
    "            zip_file.close()\n",
    "            print(f'{file_name} downloaded and extracted successfully.')\n",
    "            \n",
    "            # Calculate the total size of the files in the directory\n",
    "            total_size_gb = get_directory_size(data_folder) / (1024**3)\n",
    "            \n",
    "            # Check if the total size exceeds the maximum limit\n",
    "            if total_size_gb > max_directory_size_gb:\n",
    "                print(f'Total directory size exceeded. Removing oldest files.')\n",
    "                remove_oldest_files(data_folder, max_directory_size_gb)\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f'Invalid ZIP file: {file_name}. Skipping extraction.')\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49080223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-21T12:15:06.699340Z",
     "iopub.status.busy": "2023-06-21T12:15:06.698225Z",
     "iopub.status.idle": "2023-06-21T12:15:06.704899Z",
     "shell.execute_reply": "2023-06-21T12:15:06.703524Z"
    },
    "papermill": {
     "duration": 0.012271,
     "end_time": "2023-06-21T12:15:06.707338",
     "exception": false,
     "start_time": "2023-06-21T12:15:06.695067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_directory_size(directory):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total_size += os.path.getsize(fp)\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c32b089",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-21T12:15:06.713639Z",
     "iopub.status.busy": "2023-06-21T12:15:06.713227Z",
     "iopub.status.idle": "2023-06-21T12:15:06.722576Z",
     "shell.execute_reply": "2023-06-21T12:15:06.721266Z"
    },
    "papermill": {
     "duration": 0.015052,
     "end_time": "2023-06-21T12:15:06.724795",
     "exception": false,
     "start_time": "2023-06-21T12:15:06.709743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_oldest_files(directory, max_directory_size_gb):\n",
    "    file_list = []\n",
    "    total_size_gb = 0\n",
    "\n",
    "    # Collect information about files in the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_list.append((file_path, os.path.getctime(file_path)))\n",
    "            total_size_gb += os.path.getsize(file_path) / (1024**3)\n",
    "\n",
    "    # Sort the files by creation time (oldest first)\n",
    "    file_list.sort(key=lambda x: x[1])\n",
    "\n",
    "    # Remove files until the directory size is within the limit\n",
    "    while total_size_gb > max_directory_size_gb and file_list:\n",
    "        file_path, _ = file_list.pop(0)\n",
    "        file_size_gb = os.path.getsize(file_path) / (1024**3)\n",
    "        total_size_gb -= file_size_gb\n",
    "\n",
    "        # Remove the file and its corresponding extracted CSV files\n",
    "        os.remove(file_path)\n",
    "        csv_folder = os.path.join(directory, 'csv_files')\n",
    "        csv_file_prefix = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        for file in os.listdir(csv_folder):\n",
    "            if file.startswith(csv_file_prefix):\n",
    "                csv_file_path = os.path.join(csv_folder, file)\n",
    "                os.remove(csv_file_path)\n",
    "\n",
    "        print(f'Removed file: {file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942d8bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-21T12:15:06.730909Z",
     "iopub.status.busy": "2023-06-21T12:15:06.730491Z",
     "iopub.status.idle": "2023-06-21T12:15:20.317711Z",
     "shell.execute_reply": "2023-06-21T12:15:20.316168Z"
    },
    "papermill": {
     "duration": 13.592766,
     "end_time": "2023-06-21T12:15:20.319787",
     "exception": false,
     "start_time": "2023-06-21T12:15:06.727021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202306-divvy-tripdata.zip not found. Downloading previous month data.\n",
      "Invalid ZIP file: 202306-divvy-tripdata.zip. Skipping extraction.\n",
      "202305-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202304-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202303-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202302-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202301-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202212-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202211-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202210-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202209-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202208-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202207-divvy-tripdata.zip downloaded and extracted successfully.\n",
      "202206-divvy-tripdata.zip downloaded and extracted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Specify the URL for downloading the data files\n",
    "url = 'https://divvy-tripdata.s3.amazonaws.com/{0}{1:02d}-divvy-tripdata.zip'\n",
    "\n",
    "# Set the oldest file month and year\n",
    "oldest_year = 2020\n",
    "oldest_month = 4\n",
    "\n",
    "# Set the number of months to download\n",
    "num_months_to_download = 12\n",
    "\n",
    "# Set the maximum directory size in GB\n",
    "max_directory_size_gb = 15\n",
    "\n",
    "# Call the function to download and manage the data files\n",
    "download_and_unzip_data(url, num_months_to_download, max_directory_size_gb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24.004904,
   "end_time": "2023-06-21T12:15:21.145727",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-06-21T12:14:57.140823",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
